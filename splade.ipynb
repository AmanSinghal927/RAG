{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58ef6d32-8bf9-4acb-8e74-8bead91e7913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from src.dataloader import DataLoader\n",
    "from utils.utils import flatten_list, write_list_to_file, read_list_from_file, preprocess_text\n",
    "from bs4 import BeautifulSoup\n",
    "from src.faiss.flat_idx import FlatIdx\n",
    "from src.encoder.dragon import Encoder\n",
    "from src.post_processing import idk\n",
    "import pandas as pd\n",
    "from src.eval import Eval\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b35a15a-a9a9-47db-a6ff-1cc6be4309a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/naver/splade.git"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/naver/splade.git 'C:\\Users\\J C SINGLA\\AppData\\Local\\Temp\\pip-req-build-pj0l1_8s'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [51 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\implementations\n",
      "      creating build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-312\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-312\\tokenizers\\tools\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://github.com/naver/splade.git to c:\\users\\j c singla\\appdata\\local\\temp\\pip-req-build-pj0l1_8s\n",
      "  Resolved https://github.com/naver/splade.git to commit 6a1b4bda1cdd5b7117b8387eb831db6d7ce49eeb\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers==4.18.0 (from SPLADE==2.1)\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n",
      "     ---------------------------------------- 0.0/70.3 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/70.3 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 30.7/70.3 kB 325.1 kB/s eta 0:00:01\n",
      "     --------------------------------- ---- 61.4/70.3 kB 469.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 70.3/70.3 kB 319.5 kB/s eta 0:00:00\n",
      "Collecting omegaconf==2.1.2 (from SPLADE==2.1)\n",
      "  Using cached omegaconf-2.1.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting antlr4-python3-runtime==4.8 (from omegaconf==2.1.2->SPLADE==2.1)\n",
      "  Using cached antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from omegaconf==2.1.2->SPLADE==2.1) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\j c singla\\appdata\\roaming\\python\\python312\\site-packages (from transformers==4.18.0->SPLADE==2.1) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.18.0->SPLADE==2.1)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.18.0->SPLADE==2.1)\n",
      "  Downloading tokenizers-0.12.1.tar.gz (220 kB)\n",
      "     ---------------------------------------- 0.0/220.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 220.7/220.7 kB 6.8 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from transformers==4.18.0->SPLADE==2.1) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0->SPLADE==2.1) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.18.0->SPLADE==2.1) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\j c singla\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers==4.18.0->SPLADE==2.1) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from requests->transformers==4.18.0->SPLADE==2.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from requests->transformers==4.18.0->SPLADE==2.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from requests->transformers==4.18.0->SPLADE==2.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from requests->transformers==4.18.0->SPLADE==2.1) (2024.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from sacremoses->transformers==4.18.0->SPLADE==2.1) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\j c singla\\miniconda3\\lib\\site-packages (from sacremoses->transformers==4.18.0->SPLADE==2.1) (1.3.2)\n",
      "Using cached omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
      "Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.5/4.0 MB 15.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.8/4.0 MB 10.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/4.0 MB 11.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.9/4.0 MB 11.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 2.4/4.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.9/4.0 MB 11.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.2/4.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.7/4.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.0/4.0 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 10.6 MB/s eta 0:00:00\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "   ---------------------------------------- 0.0/897.5 kB ? eta -:--:--\n",
      "   ----------------- --------------------- 409.6/897.5 kB 12.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 593.9/897.5 kB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 897.5/897.5 kB 7.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: SPLADE, antlr4-python3-runtime, tokenizers\n",
      "  Building wheel for SPLADE (setup.py): started\n",
      "  Building wheel for SPLADE (setup.py): finished with status 'done'\n",
      "  Created wheel for SPLADE: filename=SPLADE-2.1-py3-none-any.whl size=65950 sha256=8940f4f7639a770851e93ce1c7d5e4182f3463c9ff15beeb2949133216167faa\n",
      "  Stored in directory: C:\\Users\\J C SINGLA\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-c55fxi59\\wheels\\21\\80\\b6\\91a123d2e2c9186be845fe411605493e2ab0c6c68bdbe21a5c\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): started\n",
      "  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141217 sha256=fd808fa4ad67f989196cd15bbcba606f1ee726a64530373dce82c1770c6fc082\n",
      "  Stored in directory: c:\\users\\j c singla\\appdata\\local\\pip\\cache\\wheels\\3e\\92\\b7\\08c6a108fc5bf6370a7540d11bbe9befc99b7e045ac7558d49\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Successfully built SPLADE antlr4-python3-runtime\n",
      "Failed to build tokenizers\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/naver/splade.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074052e-825b-4adf-83d8-f55cf5052cbc",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0154e238-4f03-4696-b208-6fc2e0868c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r\"C:\\Users\\J C SINGLA\\Downloads\\External - take_home_challenge_(withJSONs)\\take_home_challenge_(withJSONs)\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b37e9a73-c86e-410b-b869-31b7a7d3f757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "all_data_sherpa = read_list_from_file(save_path, \"sherpa_paras_and_tables\")\n",
    "filenames_sherpa = read_list_from_file(save_path, \"sherpa_paras_and_tables_filenames\")\n",
    "assert (len(all_data_sherpa)==len(filenames_sherpa))\n",
    "print (len(all_data_sherpa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25bdd818-eb32-4085-857a-203dfd616375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = r\"C:\\Users\\J C SINGLA\\Downloads\\External - take_home_challenge_(withJSONs)\\take_home_challenge_(withJSONs)\\document_questions.xlsx\"\n",
    "ground_truth = pd.read_excel(ground_truth_path)\n",
    "ground_truth_text = ground_truth[ground_truth[\"complexity\"].isin([\"no raw_text\", \"table\", \"text\"])].copy()  \n",
    "test_data = list(ground_truth_text[\"relevant questions\"])\n",
    "test_labels = list(ground_truth_text[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3ab24e-e717-4524-8e1a-760568c48380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3139f-50b8-44b8-96cf-4ec3b0656b1b",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e12198a2-d921-4918-a645-cac8dd4094d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model_id = 'naver/splade-cocondenser-ensembledistil'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edefb7-78ef-4940-b901-268052352e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081b92d3-188a-421a-b1f1-df891d1eb681",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
